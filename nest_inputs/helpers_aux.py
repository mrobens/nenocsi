#************************************************************************************
#                                                                                    
# NENoCSi-Framework: Network traffic within NENoCSi generated by NEST
# Copyright (C) 2022-2023 Forschungszentrum Juelich GmbH, ZEA-2                           
# Author: Markus Robens <https://www.fz-juelich.de/profile/robens_m>                 
#                                                                                    
# This program is free software: you can redistribute it and/or modify               
# it under the terms of the GNU General Public License as published by               
# the Free Software Foundation, either version 3 of the License, or                  
# (at your option) any later version.                                                
#                                                                                    
# This program is distributed in the hope that it will be useful,                    
# but WITHOUT ANY WARRANTY; without even the implied warranty of                     
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the                      
# GNU General Public License for more details.                                       
#                                                                                    
# You should have received a copy of the GNU General Public License                  
# along with this program.  If not, see <https://www.gnu.org/licenses/>.             
#                                                                                    
#************************************************************************************
#                                                                                    
# This is an auxiliary file that augments the functions in helpers.py of the         
# NEST cortical microcircuit model by functions to produce YAML files that           
# can be read into NENoCSi to assess traffic caused by this model in the             
# physical interconnect.                                                             
#                                                                                    
#************************************************************************************

import nest
import yaml
from collections import defaultdict
import os
import helpers
import numpy as np

def __get_target_neurons(pop):
    """ Creates a dictionary that uses source neuron IDs as keys and
        lists of target neurons as values

        Parameters
        ----------
        pop
            NEST NodeCollection representing a population

        Returns
        -------
        target_neurons
            Dictionary containing source neurons as keys and lists of target
            neurons as values
    """
    target_neurons = defaultdict(list)
    conns = nest.GetConnections(pop)
    src_trgt_nrns = zip(conns.get('source'), conns.get('target'))
    for x,y in src_trgt_nrns:
        target_neurons[x].append(y)
    return dict(target_neurons)

def __get_node_ID_4_neuron(nID, npCU, id_array, pop_sizes):
    """ Determines the network node ID corresponding to a neuron ID
        
        Parameters
        ----------
        nID
            Neuron ID
        npCU
            Number of neurons that can be hosted by a network node
        id_array
            Numpy-array of neuron IDs indicating the boundaries of populations
        pop_sizes
            Numpy-array of population sizes in terms of neurons 
        
        Returns
        -------
        nodeID
            Node ID of the node hosting the neuron as int
    """
    nodeID = None
    n_offset = 0
    for i in range(0, id_array.shape[0]):
        if ((nID >= id_array[i][0]) and (nID <= id_array[i][1])):
            nd_in_pop = np.floor((nID - id_array[i][0]) / npCU)
            nodeID = n_offset + nd_in_pop
            return int(nodeID.item())
        n_offset += np.ceil(pop_sizes[i] / npCU)
    return nodeID

def Export_Spikes_and_Targets(path, name, exp_name, pops, pop_sizes, npCU, begin, end):
    """ Generates a dictionary of hardware network nodes, their spike times, and their
        targets within a YAML file named according to the specification given in exp_name

        Paratmeters
        -----------
        path
            Path where spike_recorder files are stored
        name
            Name of the spike recorders
        exp_name
            Name of the YAML output file(s) 
            ('_NpN_', str(npCU), and '.yaml' will be appended for UC and LMC;
             '_NpN_', str(npCU), and '_LMC_SRC.yaml' will be appended for LMC_SRC) 
        pops
            List of NEST NodeCollections representing neuron populations
        pop_sizes
            Numpy-array of population sizes in terms of neurons
        npCU
            Number of neurons hosted by a network node
        begin
            Time point (in ms) to start loading spike times (included)
        end
            Time point (in ms) to stop loading spike times (included)
    """
    ###################################################################
    # As an extension to the initial approach, now two output files   #
    # are generated. The original file is intended for examination of #
    # destination-address based routing approaches, while the second  #
    # file enables experiments regarding source-address based routing #
    # --------------------------------------------------------------- #
    # Abbreviations used above and below:                             #
    #    UC:      unicast (detination-address based)                  #
    #    LMC:     destination-address based local multicast           #
    #    LMC_SRC: source-address based local multicast                #
    ################################################################### 
    # UC, LMC
    fn1 = os.path.join(path, exp_name + '_NpN_' + str(npCU) + '.yaml')
    # LMC_SRC
    fn2 = os.path.join(path, exp_name + '_NpN_' + str(npCU) + '_LMC_SRC.yaml')
    # All
    _, pop_ids, data = helpers.__load_spike_times(path, name, begin, end)
    it = 1
    print("[        ]\b\b\b\b\b\b\b\b\b", end='', flush = True)
    for i, pop in enumerate(pops):
        # UC, LMC
        spike_t_trgt_dict = {}
        # LMC_SRC
        spike_t_trgt_dict_lmc = {}
        # All
        src_trgt_ns = __get_target_neurons(pop)
        pop_dat = data[i]
        for j in range(pop_dat.shape[0]):
            src_nd = __get_node_ID_4_neuron(pop_dat[j]['sender'],npCU, pop_ids, pop_sizes)
            t_curr = float("%.1f" % (pop_dat[j]['time_ms'].item() - begin))
            if (not src_nd in spike_t_trgt_dict): # Simultaneously applies to spike_t_trgt_dict_lmc as well
                trgt_ns = src_trgt_ns[pop_dat[j]['sender']]
                trgt_nds = [__get_node_ID_4_neuron(x, npCU, pop_ids, pop_sizes) for x in trgt_ns]
                trgt_nds = list(filter(lambda x: (x != None) and (x != src_nd), trgt_nds))
                # UC, LMC
                trgt_nd_spk_cnt = defaultdict(int)
                for k in trgt_nds:
                    trgt_nd_spk_cnt[k] += 1
                trgt_nd_spk_cnt = dict(trgt_nd_spk_cnt)
                t_trgt_nd_spk_cnt = {}
                t_trgt_nd_spk_cnt[t_curr] = trgt_nd_spk_cnt
                spike_t_trgt_dict[src_nd] = [t_trgt_nd_spk_cnt]
                # LMC_SRC (only the source address is communicated per target node)
                trgt_nd_spk_cnt_lmc = {}
                for k in set(trgt_nds):
                    trgt_nd_spk_cnt_lmc[k] = 1
                t_trgt_nd_spk_cnt_lmc = {}
                t_trgt_nd_spk_cnt_lmc[t_curr] = trgt_nd_spk_cnt_lmc
                spike_t_trgt_dict_lmc[src_nd] = [t_trgt_nd_spk_cnt_lmc]
            # Note: data is sorted chronologically; if t_curr is not yet present in spike_t_trgt_dict,
            #       there can also be no entries for target nodes and spike counts.
            #       The condition for spike_t_trgt_dict simulatneously applies to spike_t_trgt_dict_lmc.
            elif (not t_curr == list(spike_t_trgt_dict[src_nd][-1].keys())[0]):
                trgt_ns = src_trgt_ns[pop_dat[j]['sender']]
                trgt_nds = [__get_node_ID_4_neuron(x, npCU, pop_ids, pop_sizes) for x in trgt_ns]
                trgt_nds = list(filter(lambda x: (x != None) and (x != src_nd), trgt_nds))
                # UC, LMC
                trgt_nd_spk_cnt = defaultdict(int)
                for k in trgt_nds:
                    trgt_nd_spk_cnt[k] += 1
                trgt_nd_spk_cnt = dict(trgt_nd_spk_cnt)
                t_trgt_nd_spk_cnt = {}
                t_trgt_nd_spk_cnt[t_curr] = trgt_nd_spk_cnt
                spike_t_trgt_dict[src_nd].append(t_trgt_nd_spk_cnt)
                # LMC_SRC (only the source address is communicated per target node)
                trgt_nd_spk_cnt_lmc = {}
                for k in set(trgt_nds):
                    trgt_nd_spk_cnt_lmc[k] = 1
                t_trgt_nd_spk_cnt_lmc = {}
                t_trgt_nd_spk_cnt_lmc[t_curr] = trgt_nd_spk_cnt_lmc
                spike_t_trgt_dict_lmc[src_nd].append(t_trgt_nd_spk_cnt_lmc)
            else:
                # Source node and time stamp already exist - there may be new target nodes,
                # or counts for existing ones need to be updated in case of UC and LMC.
                # In case of LMC_SRC, either a new target node is added by the current
                # source neuron ID, or the count of an existing one has to be increased by
                # one, since an additional source neuron ID needs to be communicated.
                trgt_ns = src_trgt_ns[pop_dat[j]['sender']]
                trgt_nds = [__get_node_ID_4_neuron(x, npCU, pop_ids, pop_sizes) for x in trgt_ns]
                trgt_nds = list(filter(lambda x: (x != None) and (x != src_nd), trgt_nds))
                # UC, LMC
                trgt_nd_spk_cnt = defaultdict(int)
                for k in trgt_nds:
                    trgt_nd_spk_cnt[k] += 1
                trgt_nd_spk_cnt = dict(trgt_nd_spk_cnt)
                for x in trgt_nd_spk_cnt:
                    if (x in spike_t_trgt_dict[src_nd][-1][t_curr]):
                        spike_t_trgt_dict[src_nd][-1][t_curr][x] += trgt_nd_spk_cnt[x]
                    else:
                        spike_t_trgt_dict[src_nd][-1][t_curr][x] = trgt_nd_spk_cnt[x]
                # LMC_SRC (only the source address is communicated per target node)
                trgt_nd_spk_cnt_lmc = {}
                for k in set(trgt_nds):
                    trgt_nd_spk_cnt_lmc[k] = 1
                for x in trgt_nd_spk_cnt_lmc:
                    if (x in spike_t_trgt_dict_lmc[src_nd][-1][t_curr]):
                        # There has been an entry caused by another source neuron
                        spike_t_trgt_dict_lmc[src_nd][-1][t_curr][x] += 1
                    else:
                        # There has not been an entry cause by another source neuron so far
                        spike_t_trgt_dict_lmc[src_nd][-1][t_curr][x] = 1
        print('>', end='', flush = True)
        if (it == 1):
            with open(fn1, 'wt') as f1:
                yaml.dump(spike_t_trgt_dict, f1)
            with open(fn2, 'wt') as f2:
                yaml.dump(spike_t_trgt_dict_lmc, f2)
        else:
            with open(fn1, 'at') as f1:
                yaml.dump(spike_t_trgt_dict, f1)
            with open(fn2, 'at') as f2:
                yaml.dump(spike_t_trgt_dict_lmc, f2)
        it += 1
    print('')
